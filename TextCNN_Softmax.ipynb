{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "781149dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python38\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.1+cpu\n",
      "0.10.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02174845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comments']\n",
      "['여러분', '은', '지금', '서울대', '공과대학', '박사학위', '취득', '후', '글', '쓰고', '자기', '팬미팅', '자기', '가', '열고', '거기', '서', '의사', '여친', '만나', '결혼', '하고', '아이', '사준', '트램펄린', '위', '에서', '더브', '덤블링', '하다', '척추', '나가고', '회복', '후', '육아', '만화', '그리다가', '암', '걸리고', '완치', '받은', '작가', '님', '을', '보고', '계십니다', '.']\n",
      "['작가', '님', '암', '4', '기', '판정', '받았다고요', '?', '완치', '?']\n",
      "['미쳤다', '작가', '님', '오셨다', '작가', '님', '육아', '일기', '할', '때', '박사', '과정', '이었는데', '저', '박사', '학위', '땄습니다', '.', '작가', '님', '이', '왜', '산', '의', '정상', '에', '오르냐의', '이유', '를', '표현', '하신', '적', '있었는데', '할', '수', '있다를', '나', '는', '해냈다로', '바꾸기', '위', '해서라', '하셨죠', '.', '그', '말', '마음속', '에', '깊이', '새기고', '힘낼', '수', '있었습니다', '.', '너무', '괴롭고', '힘들었는데', '해냈습니다', '.']\n",
      "['작가', '님', '프롤로그', '부터', '울리시기', '있나요', '?', 'ㅜㅜ', '너무', '기다렸어요', '!', '닥액닥', '속편', '같은', '느낌', '으로', '시작', '해서', '더', '좋네요', '!', '이번', '작도', '기대하며', '보겠습니다', '!']\n",
      "['죽음', '의', '경지', '를', '두', '번', '이나', '극복', '하신', '정대만', '처럼', '포기', '를', '모르는', '닥터', '베르', '님', '의', '화려한', '복귀', '를', '진심', '으로', '환영', '합니다']\n",
      "['진짜', '너무', '보고', '싶었어요', '작가', '님', 'ㅜㅜ', '♡']\n",
      "['하씨', '이', '..', '이미', '알', '고', '있는데도', '눈물', '난다', '...', '그래도', '작가', '님', '픽', '해준', '네이버', '에겐', '사랑', '을', '...', '♥']\n",
      "['보고', '싶었어요', 'ㅠㅠ', '눈물', '광광', '제', '눈물', '받고', '올라가세요', 'ㅠㅠ']\n",
      "['프롤로그', '부터', '나를', '울리는', '작가', '가', '있다', '?!']\n",
      "['베도', '때', '부터', '응원', '했어요', '작가', '님', '.', '암', '밍아웃', '하셨던', '회차', '를', '본', '충격', '이', '여전히', '기억', '에', '남아', '요', '.', '저', '는', '부부', '가', '독일', '에서', '박사', '과정', '을', '밟고', '있고', '지난', '12월', '에', '저', '도', '갑산', '성암', '판정', '을', '받았어요', '착한', '암', '이', '라지', '만', '그', '충격', '은', '글', '로', '읽는다고', '이해', '되는', '게', '아니었어요', '.', '작가', '님', '이', '서술', '하신', '암', '환자', '의', '감정', '단계', '를', '고스', '란', '히', '겪고', '이', '젠', '좀', '평온한', '마음', '으로', '수술', '을', '기다리고', '있습니다', '.', '수술', '후', '에', '졸업', '과', '임신', '이', '평탄하게', '될까', '두렵지만', '이렇게', '살아내시는', '작가', '님', '의', '삶', '이', '제', '게', '응원', '이', '되었어요', '.', '종종', '더', '많이', '작가', '님', '의', '이전', '작품', '이', '생각', '이', '났답니다', '.', '그땐', '아무', '것', '도', '이해', '하지', '못', '하고', '봤던', '거구', '나', '겪지', '않으면', '모르는', '것', '들', '이', '있구나', '.', '이번', '작품', '을', '통해', '서', '저', '도', '위로', '받고', '응원', '받길', '기대', '해봅니다', '화이팅', '!']\n",
      "['선', '댓글', '달', '고', '웹툰', '봤다가', '눈물', '나요']\n",
      "['동물', '을', '의인화', '시켜', '세상', '살이', '를', '현실', '적', '으로', '묘사', '해', '많은', '독자', '들', '에게', '공감', '받았던', '육아', '일기', '에', '이어', '이번', '엔', '조금', '은', '무거우면서', '진지한', '병원', '일기', '로', '세상', '살이', '에', '지쳐', '상처', '입고', '병든', '사람', '들', '마음', '을', '치유', '해주는', '하트', '닥터', '(', '마음', '의사', ')', '닥터', '베르', '작가', '님', '컴백', '을', '환영', '합니다', '!']\n",
      "['그래도', '완치', '하셨다는', '결말', '알아서', '다행', '이지', '진짜', '너무', '슬퍼요']\n",
      "['육아', '일기', '때', '부터', '봐', '왔는데', '그림', '체', '발전', '한', '게', '눈', '에', '보이네요', '좋은', '작품', '감사', '드립니다', ':)']\n",
      "['아이고', '울지마', '레', '서', '작가', '님', '지금', '처럼', '앞', '으로도', '건', '추건', '혈', '!', '차기작', '대박', '과', '함께', '응원', '하겠습니다']\n",
      "['작가', '님', '뒷', '배경', '꼼꼼히', '안', '그려도', '좋아요', '그냥', '캐릭터', '만', '그려도', '좋아요', '그냥', '다', '좋아요', '육아', '일기', '는', '제', '인생', '템', '이에요', '너무', '너무', '환영', '해요', '~~^^']\n",
      "['웹툰', '연재', '가', '그렇게', '힘들다던데', '웹툰', '보단', '건강', '이', '우선', '이에요', '.', '다른', '데선', '반대', '로', '말', '하고', '다니는', '거', '같지만']\n",
      "['유', '퀴즈', '유재석', '님', '이', '메뚜기', '로', '조세호', '님', '이', '비버', '로', '나왔는데', '이', '언급', '이', '별로', '없네요', 'ㅋㅋㅋ']\n",
      "['아이고', '...', '레', '서', '많이', '놀랐겠네요', 'ㅠㅠ']\n",
      "['분량', '보소', '...', '작가', '님', '얼마나', '준비', '하신', '거', '예요', '대체', '?', '키', '패드', '를', '갈았네요', '아주', '.', '작가', '님', '완전', '사', '...', '사', '...', 'ㄹ']\n",
      "['참', '선', '심하게', '넘으셨었어요', 'ㅎㅎ', '많이', '들', '걱정', '했었다는', '...']\n",
      "['ㅋㅋㅋ', '아', '이', '날', '기억나요', 'ㅋㅋㅋ', '다', '들', '작가', '님', '문', '열어', '보라', '고', 'ㅋㅋㅋ']\n",
      "['상담', '의', '결과', '였다니', '!!!', '...', '공대생', '의', '인풋', '아웃', '풋', '이란', '...', '허헣']\n",
      "['ㅋㅋㅋ', '대국민', '낚시', '사건', '이', '이렇게', 'ㅋㅋㅋ', '탄생', '했군요']\n",
      "['아아', 'ㅋㅋㅋ', '진짜', '놀랐었다구요', '...', 'ㅠㅠ']\n",
      "['아이', 'ㅋㅋㅋ', 'ㅠㅠ']\n",
      "['아', '그래서', '그때', '...', '그러셨군요', '암', '쏘', '새', '드는', '존심', '상하', '지만', '웃겼어요']\n",
      "['ㅋㅋㅋ', '저기', '암', '환자', '만의', '드립', '력', '때문', '에', '열', '받지만', '티', '는', '못', '내고', '소심하게', '별', '점', '테러', '한', '구', '독자', '요기', '요', '...', 'ㅋㅋㅋ']\n",
      "['아', '진짜', '저', '때', '저랬지', '~', '하고', '웃', '을', '수', '있어서', '너무', '행복해요', '작가', '님', '근데', '문', '잠깐', '만', '열어주세요', '아', '택배', '라니까', '?']\n",
      "['아아', 'ㅋㅋㅋ', '진짜', '놀랐었다구요', '...', 'ㅠㅠ']\n",
      "['보는', '내내', '불안해', '죽는줄', '알았다']\n",
      "['비극', '이다']\n",
      "['웹툰', '제목', '이', '모나', '잔', '혹사', '인가요', '?', '불쌍한', '모나', 'ㅠㅠ']\n",
      "['박주연', '이', '제일', '나쁨', '.', '못', '난', '게', '욕심', '만', '많고', '지', '잘못', '은', '생각', '안', '하고', '남', '탓', '만', '함', '.']\n",
      "['차', '미미', '정도', '로', '돌아있는', '사람', '도', '박주연', '이', '정상', '이', '아니라는', '걸', '알아보네']\n",
      "['나', '만', '박주연', '이', '제일', '싫음', '...?', 'ㅠㅠ']\n",
      "['와', '진짜', '파란', '분위기', '가', '스릴', '감', '을', '더하네요']\n",
      "['와', '진짜', '불쾌하다']\n",
      "['어질어질하다', '...']\n",
      "['내', '가', '볼', '때', '박주연', '남편', '이', '박주연', '한테', '불리한', '진술', '할', '것', '같은데', '?']\n",
      "['와', '박주연', '개', '싫어', '진짜', 'ㅋㅋ']\n",
      "['주인공', '진짜', '이상하면서도', '불쌍하면서도', '...', '모르겠다', '이', '복잡', '미묘한', '감정', '...']\n",
      "['학폭', '가해자', '들', '이', '저렇게', '사는', '거', '진짜', '보기', '싫다', '...']\n",
      "['주인공', '찌질하긴', '한데', '불쌍하다', 'ㅜㅜ']\n",
      "['사건', '은', '아무', '것', '도', '시작', '하지', '않았는데', '벌써', '부터', '가슴', '이', '답답하고', '불안하다', '...', 'ㄷㄷ']\n",
      "['애', '얼굴', '기괴하게', '보정', '떡칠', '해놓고', '올렸는데', '라', '방', '에서', '들켰네', '...']\n",
      "['뭔가', '불안하다', '...', '채린', '이', '가', '뭔', '짓', '해서', '난리', '날', '듯', '...']\n",
      "['뭔가', '불편해서', '보기', '싫은데', '자꾸', '보게', '됨', '...']\n",
      "['제임스', '가', '아뇨', '거짓말', '이야', '할', '때', '표정', '뭔가', '섬뜩함']\n",
      "['주인공', '커플', '베드', '신', '이', '이렇게', '까지', '고통스러웠던', '건', '처음', '이네']\n",
      "['예', '희가', '언제', '망할까', '보다', '주연', '이', '언제', '망할까', '더', '기다려', '짐']\n",
      "['의사', '남친', '걍', '마마', '보이', '에', '짠', '돌이', '자', '너', '...', '걍', '최악', '인데', '...']\n",
      "['짚이는', '게', '많아서', '더', '당황', '스럽고', '무서운', '듯', '와중', '에', '골프채', '라니', '...', '아무리', '화나도', '그건', '아니지']\n",
      "['너무', '역겨운', '베드', '신', '이다', '...']\n",
      "['모든', '러브', '라인', '이', '이렇게', '까지', '비호감', '인', '만화', '는', '처음', '이다', '..']\n",
      "['진짜', '정상', '이', '없네', '정상', '이', '없어', 'ㅋㅋ', '호빠남', '한테', '정신', '못', '차리는', '브릣', '니', '...', '그', '호빠남', '소개', '해', '주고', '돈', '뜯는', '예', '희', '..', '불륜', '남녀', '와', '남', '의', '집', '도촬', '하는', '나래', '엄마', '..']\n",
      "['채린', '이', '가', '너무', '안쓰러', '움', '...', '한창', '애착', '형성', '할', '시기', '에', '엄마', '아빠', '둘', '다', '관심', '안', '줘서', '애착', '형성', '이', '전혀', '안', '됐네']\n",
      "['서로', '가', '서로', '의', '악', '이네', '...']\n",
      "['박주연', '이나', '김예', '희나', '둘', '다', '서로', '한테', '가해자', '이자', '피해자', '이네', 'ㅋㅋ', '누가', '불쌍하다', '할', '거', '없이', 'ㅋㅋ', '근데', '예', '희는', '좀', '위험해', '보인다']\n",
      "['저런', '엄마', '밑', '에서', '자랄', '채린', '이', '가', '불쌍하다']\n",
      "['만화가', '재밌어요', '웃겨요', 'ㅋㅋㅋ', '보고', '나서', '행복해지네요', '한', '주', '화이팅', '!']\n",
      "['아', 'ㅋㅋㅋ', '너무', '조아', '너무', '달달', '해', '!!!']\n",
      "['연애', '하고', '결혼', '해서', '살찌면', '행복한', '거래', '요', '!', '수', '달님', '홀', '앙', '이', '님', '은', '역시', '천생연분', '이신', '듯', 'ㅋㅋㅋ']\n",
      "['ㅋㅋㅋ', '아', '진짜', '귀여워', '마지막', '뭐', '여', 'ㅠㅠ']\n",
      "['ㅋㅋㅋ', '무슨', '로맨스', '영화', '같이', '연애', '하셨네요']\n",
      "['이', '가족', '누굴', '닮든', '너무', '사랑스럽다', '이', '거', '에', '용', '~']\n",
      "['원래', '결제', '잘', '안', '하는데', '너무', '너무', '귀엽고', '재밌어서', '결제', '했어용', '저', '의', '600원', '이', '수', '랑', '이의', '포클레인', '값', '이', '되', '기를', '...']\n",
      "['수', '랑', '이', '너무', '귀엽다', '이', '거', '에', '용']\n",
      "['보노보노', '가', '생각나는', '따뜻한', '그림', '체', '넘', '좋아']\n",
      "['버섯', '먹는', '거', '너무', '귀여워', '...']\n",
      "['에', '휴', '채훈', '이', '제일', '역겹네']\n",
      "['민아', '도', '이렇게', '보', '니까', '..', '참', '불쌍하게', '살았네']\n",
      "['민아', '는', '수희', '한테', '죄책감', '이란', '게', '하나', '도', '없나', '봐']\n",
      "['민아', '도', '좀', '그렇긴', '한데', '채훈', '은', 'ㄹㅇ', '극혐', '이다', '...', '말', '만', '번지르르하지', '결국', '은', '자기', '합리화', '하', '는', '거', '잖음', 'ㅋㅋ']\n",
      "['민아', '진짜', '자격지심', '레전드', '네', '...', '저건', '자존감', '이', '낮은', '문제', '가', '아니라', '걍', '수희', '에', '대한', '자격지심', '이', '쩌', '는', '거임', ';']\n",
      "['수희', '주변', '에', '정상', '이', '없다']\n",
      "['배추', '도사', '진짜', '극혐', ';', '지를', '위', '한', '거', '면서', '수희', '를', '위', '한', '거야', 'ㅇㅈㄹ', ';']\n",
      "['민아', '뭐', '가', '그렇게', '혼자', '불쌍한', '거야', '?', '모든', '걸', '자기', '가', '망쳐놓았으면서']\n",
      "['이해관계', '에', '따라', '비열하고', '추악해지며', '아무', '도', '못', '믿는', '사람', '의', '유약한', '내면', '을', '입체', '적', '으로', '보여주는', '대사', '가', '압권', '이다', '.', '이', '작가', '는', '웹툰', '일', '하지', '않았으면', '정신과', '의사', '나', '심리상담가', '하셨을', '듯']\n",
      "['매니저', '도', '그렇고', '다', '안타깝다', '그냥', '...']\n",
      "['결국', '가장', '슬퍼', '해주는', '것', '도', '시청자', '네']\n",
      "['웹툰', '인데', '가슴', '이', '아릿함', '사람', '들', '의', '감정', '이', '너무', '잘', '느껴짐', '나', '만', '그래', '..?']\n",
      "['일기', '라는', '게', '원래', '저렇게', '무서운', '거', '였나', '?']\n",
      "['김신', '눈물', '흘리는', '거', '에서', '살짝', '소름', '돋음']\n",
      "['일기', '내용', '좀', '소름', '돋네', '...;']\n",
      "['두석', '이', '악역', '이었다니', '와', '...', '충격', '이다']\n",
      "['뭔가', '반전', '이', '있는', '듯', '?']\n",
      "['무섭']\n",
      "['흠', '이건', '미친', '놈이로군요']\n",
      "['주', '오', '남도', '참', '안', '됐다', '..', '엄마', '는', '아들', '생일도', '모르면서', '자기', '아들', '을', '저렇게', '무시', '하고', '있네', '...']\n",
      "['무서운', '게', '딱', '좋아에서', '가장', '끔찍하고', '무서운', '에피']\n",
      "['역대', '에피소드', '중', '가장', '소름', '끼쳐', '...', '좀', '바뀌길', '바랬', '을', '정도', '...']\n",
      "['직원', '개', '불쌍하네']\n",
      "['와', '직원', '개', '불쌍', 'ㅠㅠ', '.', '좋았는데']\n",
      "['여주', '진짜', '불쌍하다', '...']\n",
      "['어른', '되고', '나서', '보', '니까', '그러려니', '하는데', '이번', '에피', '는', '좀', '소름', '돋네', '...']\n",
      "['나도', '고마워', '미래', '가', '웃는', '모습', '보게', '해줘서']\n",
      "['희나의', '정체', '를', '알', '고', '보니', '그저', '웃겨', '보임', 'ㅋㅋㅋ']\n",
      "['백합', '이', '가', '미래', '를', '질투', '하는', '게', '딱', '보이', '네', '..']\n",
      "['미래', '마음', '이', '아프네', '..']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[comments]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[여러분, 은, 지금, 서울대, 공과대학, 박사학위, 취득, 후, 글, 쓰고, 자기...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[작가, 님, 암, 4, 기, 판정, 받았다고요, ?, 완치, ?]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[미쳤다, 작가, 님, 오셨다, 작가, 님, 육아, 일기, 할, 때, 박사, 과정,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[작가, 님, 프롤로그, 부터, 울리시기, 있나요, ?, ㅜㅜ, 너무, 기다렸어요,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>[어른, 되고, 나서, 보, 니까, 그러려니, 하는데, 이번, 에피, 는, 좀, 소...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>[나도, 고마워, 미래, 가, 웃는, 모습, 보게, 해줘서]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>[희나의, 정체, 를, 알, 고, 보니, 그저, 웃겨, 보임, ㅋㅋㅋ]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>[백합, 이, 가, 미래, 를, 질투, 하는, 게, 딱, 보이, 네, ..]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>[미래, 마음, 이, 아프네, ..]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              comments  label\n",
       "0                                           [comments]      1\n",
       "1    [여러분, 은, 지금, 서울대, 공과대학, 박사학위, 취득, 후, 글, 쓰고, 자기...      0\n",
       "2                [작가, 님, 암, 4, 기, 판정, 받았다고요, ?, 완치, ?]      0\n",
       "3    [미쳤다, 작가, 님, 오셨다, 작가, 님, 육아, 일기, 할, 때, 박사, 과정,...      0\n",
       "4    [작가, 님, 프롤로그, 부터, 울리시기, 있나요, ?, ㅜㅜ, 너무, 기다렸어요,...      0\n",
       "..                                                 ...    ...\n",
       "96   [어른, 되고, 나서, 보, 니까, 그러려니, 하는데, 이번, 에피, 는, 좀, 소...      1\n",
       "97                   [나도, 고마워, 미래, 가, 웃는, 모습, 보게, 해줘서]      0\n",
       "98             [희나의, 정체, 를, 알, 고, 보니, 그저, 웃겨, 보임, ㅋㅋㅋ]      0\n",
       "99          [백합, 이, 가, 미래, 를, 질투, 하는, 게, 딱, 보이, 네, ..]      1\n",
       "100                               [미래, 마음, 이, 아프네, ..]      1\n",
       "\n",
       "[101 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# 데이터 불러오기\n",
    "df = pd.read_csv('comments.txt', names=['comments'])\n",
    "\n",
    "# Okt 형태소 분석기 객체 생성\n",
    "okt = Okt()\n",
    "\n",
    "# 감정 사전 불러오기\n",
    "knu_dict = pd.read_csv('KNU.csv')\n",
    "knu_dict = dict(zip(knu_dict['word'], knu_dict['polarity']))\n",
    "\n",
    "# comments 칼럼을 형태소 분석한 뒤 감정 점수 계산하기\n",
    "sentiment_scores = []\n",
    "for comment in df['comments']:\n",
    "    words = okt.morphs(comment)\n",
    "    print(words)\n",
    "    score = sum([knu_dict.get(word, 0) for word in words])\n",
    "    score = score if score >= 0 else 0\n",
    "    if score > 0:\n",
    "        sentiment_scores.append(0) #일상\n",
    "    else:\n",
    "        sentiment_scores.append(1) #갈등, 자극적 성향\n",
    "\n",
    "df['comments']=df.comments.apply(okt.morphs)\n",
    "# 결과를 라벨링된 데이터프레임으로 변환하기\n",
    "comments = pd.DataFrame({'comments': df['comments'], 'label': sentiment_scores})\n",
    "\n",
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c77bb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "# Word2Vec 모델 학습\n",
    "w2v_model = Word2Vec(comments['comments'], \n",
    "                     sg=1, \n",
    "                     vector_size=100, \n",
    "                     window=2, \n",
    "                     min_count=1, \n",
    "                     workers=4)\n",
    "\n",
    "# 모델 저장\n",
    "w2v_model.wv.save_word2vec_format('word2vec')\n",
    "\n",
    "# 모델 불러오기\n",
    "w2v_model = KeyedVectors.load_word2vec_format('word2vec', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4b9a5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import RandomState\n",
    "\n",
    "rng=RandomState()\n",
    "\n",
    "tr=comments.sample(frac=0.8, random_state=rng)\n",
    "val=comments.loc[~comments.index.isin(tr.index)]\n",
    "\n",
    "tr.to_csv('train.csv', index=False, encoding='utf-8-sig')\n",
    "val.to_csv('validation.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b79e4591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import re\n",
    "from torchtext.legacy.data import Field, TabularDataset\n",
    "\n",
    "def tokenizer(text):\n",
    "    text=re.sub('[\\[\\]\\']','',str(text))\n",
    "    text=text.split(', ')\n",
    "    return text\n",
    "\n",
    "TEXT =Field(tokenize=tokenizer)\n",
    "LABEL=Field(sequential=False, use_vocab=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bfcf08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: ['ㅋㅋㅋ', '무슨', '로맨스', '영화', '같이', '연애', '하셨네요'] 0\n",
      "Val: ['comments'] 1\n",
      "81 20\n"
     ]
    }
   ],
   "source": [
    "train, validation = TabularDataset.splits(\n",
    "    path='.',\n",
    "    train='train.csv',\n",
    "    validation='validation.csv',\n",
    "    format='csv',\n",
    "    fields=[('text', TEXT), ('label', LABEL)],\n",
    "    skip_header=True\n",
    ")\n",
    "\n",
    "print(\"Train:\",train[0].text, train[0].label)\n",
    "print(\"Val:\",validation[0].text, validation[0].label)\n",
    "print(len(train),len(validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "391e2d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 벡터의 개수와 차원 : torch.Size([677, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.vocab import Vectors\n",
    "from torchtext.legacy.data import BucketIterator\n",
    "\n",
    "vectors=Vectors(name='word2vec')\n",
    "TEXT.build_vocab(train, vectors=vectors, min_freq=1, max_size=None)\n",
    "\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "vocab=TEXT.vocab\n",
    "\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_itr,validation_itr=BucketIterator.splits(\n",
    "    datasets=(train, validation),\n",
    "    batch_size=8,\n",
    "    device=device,\n",
    "    sort=False\n",
    ")\n",
    "\n",
    "print('임베딩 벡터의 개수와 차원 : {}'.format(TEXT.vocab.vectors.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "074a3e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_built, emb_dim, dim_channel, kernel_wins, num_class):\n",
    "        super(TextCNN, self).__init__()\n",
    "        \n",
    "        self.embed=nn.Embedding(len(vocab_built), emb_dim)\n",
    "        self.embed.weight.data.copy_(vocab_built.vectors)\n",
    "        \n",
    "        self.convs=nn.ModuleList([nn.Conv2d(1, dim_channel, (w, emb_dim)) for w in kernel_wins])\n",
    "        self.relu=nn.ReLU()\n",
    "        self.dropout=nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc=nn.Linear(len(kernel_wins)*dim_channel, num_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb_x=self.embed(x)\n",
    "        emb_x=emb_x.unsqueeze(1)\n",
    "        \n",
    "        con_x=[self.relu(conv(emb_x)) for conv in self.convs]\n",
    "        \n",
    "        pool_x=[F.max_pool1d(x.squeeze(-1), x.size()[2]) for x in con_x]\n",
    "        \n",
    "        fc_x=torch.cat(pool_x, dim=1)\n",
    "        fc_x=fc_x.squeeze(-1)\n",
    "        fc_x=self.dropout(fc_x)\n",
    "        \n",
    "        logit=self.fc(fc_x)\n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bc24b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_itr, optimizer):\n",
    "    model.train()\n",
    "    corrects, train_loss=0.0, 0\n",
    "    \n",
    "    for batch in train_itr:\n",
    "        \n",
    "        text, target=batch.text, batch.label\n",
    "        text=torch.transpose(text, 0, 1)\n",
    "        target.data.sub_(1)\n",
    "        text, target=text.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logit=model(text)\n",
    "        \n",
    "        loss=F.cross_entropy(logit, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss+=loss.item()\n",
    "        result=torch.max(logit, 1)[1]\n",
    "        corrects+=(result.view(target.size()).data==target.data).sum()\n",
    "        \n",
    "    train_loss/=len(train_itr.dataset)\n",
    "    accuracy=100.0*corrects/len(train_itr.dataset)\n",
    "    \n",
    "    return train_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84fae0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, device, itr):\n",
    "    model.eval()\n",
    "    corrects, test_loss=0.0, 0\n",
    "    \n",
    "    for batch in itr:\n",
    "        text=batch.text\n",
    "        target=batch.label\n",
    "        text=torch.transpose(text, 0, 1)\n",
    "        target.data.sub_(1)\n",
    "        text, target=text.to(device), target.to(device)\n",
    "        \n",
    "        logit=model(text)\n",
    "        loss=F.cross_entropy(logit, target)\n",
    "        \n",
    "        test_loss+=loss.item()\n",
    "        result=torch.max(logit, 1)[1]\n",
    "        corrects+=(result.view(target.size()).data==target.data).sum()\n",
    "        \n",
    "    test_loss/=len(itr.dataset)\n",
    "    accuracy=100.0*corrects/len(itr.dataset)\n",
    "    \n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51f061ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextCNN(\n",
      "  (embed): Embedding(677, 100)\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv2d(1, 10, kernel_size=(1, 100), stride=(1, 1))\n",
      "    (1): Conv2d(1, 10, kernel_size=(2, 100), stride=(1, 1))\n",
      "    (2): Conv2d(1, 10, kernel_size=(3, 100), stride=(1, 1))\n",
      "  )\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=30, out_features=2, bias=True)\n",
      ")\n",
      "Train Epoch:1\tLoss:0.09292919326711584\tAccuracy:56.790122985839844\n",
      "Valid Epoch:1\tLoss:0.09961494505405426\tAccuracy:70.0\n",
      "model saves at 70.0 accuracy\n",
      "--------------------------------------------------------------------------\n",
      "Train Epoch:2\tLoss:0.09206967294952016\tAccuracy:56.790122985839844\n",
      "Valid Epoch:2\tLoss:0.0990676611661911\tAccuracy:70.0\n",
      "--------------------------------------------------------------------------\n",
      "Train Epoch:3\tLoss:0.09272894815162376\tAccuracy:56.790122985839844\n",
      "Valid Epoch:3\tLoss:0.09855553209781646\tAccuracy:70.0\n",
      "--------------------------------------------------------------------------\n",
      "Train Epoch:4\tLoss:0.08986466828687692\tAccuracy:56.790122985839844\n",
      "Valid Epoch:4\tLoss:0.0978886216878891\tAccuracy:70.0\n",
      "--------------------------------------------------------------------------\n",
      "Train Epoch:5\tLoss:0.08900961022318145\tAccuracy:61.7283935546875\n",
      "Valid Epoch:5\tLoss:0.09703869223594666\tAccuracy:70.0\n",
      "--------------------------------------------------------------------------\n",
      "Train Epoch:6\tLoss:0.0887818307052424\tAccuracy:62.96296310424805\n",
      "Valid Epoch:6\tLoss:0.09568344354629517\tAccuracy:70.0\n",
      "--------------------------------------------------------------------------\n",
      "Train Epoch:7\tLoss:0.08228662646847007\tAccuracy:75.30863952636719\n",
      "Valid Epoch:7\tLoss:0.09469765722751618\tAccuracy:70.0\n",
      "--------------------------------------------------------------------------\n",
      "Train Epoch:8\tLoss:0.0809076053124887\tAccuracy:76.543212890625\n",
      "Valid Epoch:8\tLoss:0.09286904931068421\tAccuracy:70.0\n",
      "--------------------------------------------------------------------------\n",
      "Train Epoch:9\tLoss:0.07322664209354071\tAccuracy:90.12345886230469\n",
      "Valid Epoch:9\tLoss:0.08998726606369019\tAccuracy:75.0\n",
      "model saves at 75.0 accuracy\n",
      "--------------------------------------------------------------------------\n",
      "Train Epoch:10\tLoss:0.06279328207910796\tAccuracy:93.82716369628906\n",
      "Valid Epoch:10\tLoss:0.08642522096633912\tAccuracy:75.0\n",
      "--------------------------------------------------------------------------\n",
      "Train Epoch:11\tLoss:0.05218505528238085\tAccuracy:97.5308609008789\n",
      "Valid Epoch:11\tLoss:0.0817715972661972\tAccuracy:75.0\n",
      "--------------------------------------------------------------------------\n",
      "Train Epoch:12\tLoss:0.03845695948895113\tAccuracy:98.76543426513672\n",
      "Valid Epoch:12\tLoss:0.07632700204849244\tAccuracy:80.0\n",
      "model saves at 80.0 accuracy\n",
      "--------------------------------------------------------------------------\n",
      "Train Epoch:13\tLoss:0.029407484847822307\tAccuracy:98.76543426513672\n",
      "Valid Epoch:13\tLoss:0.07204698771238327\tAccuracy:85.0\n",
      "model saves at 85.0 accuracy\n",
      "--------------------------------------------------------------------------\n",
      "Train Epoch:14\tLoss:0.021331145807548805\tAccuracy:98.76543426513672\n",
      "Valid Epoch:14\tLoss:0.06736882477998733\tAccuracy:80.0\n",
      "--------------------------------------------------------------------------\n",
      "Train Epoch:15\tLoss:0.01763567065153225\tAccuracy:97.5308609008789\n",
      "Valid Epoch:15\tLoss:0.06330197900533677\tAccuracy:80.0\n",
      "--------------------------------------------------------------------------\n",
      "Train Epoch:16\tLoss:0.016289591237350746\tAccuracy:98.76543426513672\n",
      "Valid Epoch:16\tLoss:0.06127981394529343\tAccuracy:75.0\n",
      "--------------------------------------------------------------------------\n",
      "Train Epoch:17\tLoss:0.007505906009931623\tAccuracy:100.0\n",
      "Valid Epoch:17\tLoss:0.06013181656599045\tAccuracy:75.0\n",
      "--------------------------------------------------------------------------\n",
      "Train Epoch:18\tLoss:0.006795308978101353\tAccuracy:98.76543426513672\n",
      "Valid Epoch:18\tLoss:0.059013962745666504\tAccuracy:75.0\n",
      "--------------------------------------------------------------------------\n",
      "Train Epoch:19\tLoss:0.009849702870404278\tAccuracy:98.76543426513672\n",
      "Valid Epoch:19\tLoss:0.06002526804804802\tAccuracy:75.0\n",
      "--------------------------------------------------------------------------\n",
      "Train Epoch:20\tLoss:0.006425866788184202\tAccuracy:100.0\n",
      "Valid Epoch:20\tLoss:0.05865560173988342\tAccuracy:75.0\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model=TextCNN(vocab, 100, 10, [1, 2, 3], 2).to(device)\n",
    "print(model)\n",
    "\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "optimizer=optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "best_test_acc = -1\n",
    "\n",
    "for epoch in range(1, 20+1):\n",
    "    \n",
    "    tr_loss, tr_acc=train(model, device, train_itr, optimizer)\n",
    "    print('Train Epoch:{}\\tLoss:{}\\tAccuracy:{}'.format(epoch, tr_loss, tr_acc))\n",
    "    \n",
    "    val_loss, val_acc=evaluate(model, device, validation_itr)\n",
    "    \n",
    "    print('Valid Epoch:{}\\tLoss:{}\\tAccuracy:{}'.format(epoch, val_loss, val_acc))\n",
    "    \n",
    "    if val_acc > best_test_acc:\n",
    "        best_test_acc=val_acc\n",
    "        \n",
    "        print(\"model saves at {} accuracy\".format(best_test_acc))\n",
    "        torch.save(model.state_dict(), \"R_TextCNN_Best_Validation\")\n",
    "    \n",
    "    print('--------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33a6edc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data : 일반 맥락 인지\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"R_TextCNN_Best_Validation\"))\n",
    "\n",
    "def preprocess_test_data(text):\n",
    "    okt = Okt()\n",
    "    morphs = okt.morphs(text, stem=True)\n",
    "    tokens = [vocab[token] for token in morphs]\n",
    "    return tokens\n",
    "\n",
    "#test_data = preprocess_test_data(\"추한모랑 불륜 하면서 우리 윤식이 옆에 하트 붙인 쟤는 양심이 없나? 지 딴에 죄책감이라는 게 또 있나봄 태린이 보니까 피하는 거 봐라 ㅋㅋ\")\n",
    "#test_data = preprocess_test_data(\"답답하고 짜증나기는 처음이다. 답답한 남자와 자기 세상 밖에 모르는 이기적인 여자의 얘기네\")\n",
    "#test_data = preprocess_test_data(\"주인공이 참 예쁘네. 말꼬라지가\")\n",
    "#test_data=preprocess_test_data(\"주인공 참 예쁜데 머리가 꽃밭인가봐\")\n",
    "#test_data = preprocess_test_data(\"주인공 넘 행복해 보인다 정말 다행이야 작가님 화이팅!\")\n",
    "test_data = preprocess_test_data(\"ㅋㅋㅋ 진짜 남주 넘넘 귀여워 강아지같아\")\n",
    "#test_data=preprocess_test_data(\"이 웹툰 힐링 웹툰임 ㅋㅋㅋ 사랑스러워 등장인물들 다\")\n",
    "#test_data=preprocess_test_data(\"개웃겨 ㅋㅋㅋ 진짜 어이 없는데 웃기다\")\n",
    "\n",
    "test_tensor = torch.LongTensor(test_data).unsqueeze(dim=0).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(test_tensor)\n",
    "    pred = output.argmax(dim=-1).item()\n",
    "\n",
    "if pred == 1:\n",
    "    print(\"test data : 일반 맥락 인지\")\n",
    "else:\n",
    "    print(\"test data : 갈등 맥락 인지\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Nlp)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
